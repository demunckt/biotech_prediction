{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1df5ed-7fa0-49e8-a968-c5c681de3ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "# Local model screening identified Linear Regression as goto option for Affinity and Random Forest for activity. \n",
    "# For simplicity Random Forest Regressor and classification were used.\n",
    "%%writefile train.py\n",
    "import argparse, os, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"üß™ Script started...\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=None)\n",
    "    parser.add_argument('--target_type', type=str, default='affinity')\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # 1. LOAD DATA\n",
    "    df = pd.read_parquet(os.path.join(args.train, 'data.parquet'))\n",
    "    \n",
    "    # 2. SEPARATE FEATURES AND TARGET\n",
    "    # Identify the target column based on the job type\n",
    "    target_col = 'binding_affinity' if args.target_type == 'affinity' else 'active'\n",
    "    y = df[target_col]\n",
    "\n",
    "    # 3. BULLETPROOF FEATURE SELECTION\n",
    "    # We drop the targets AND any column that contains strings (like PID_149)\n",
    "    # This ensures only numeric chemistry data goes into the model\n",
    "    X = df.drop(['binding_affinity', 'active'], axis=1, errors='ignore')\n",
    "    X = X.select_dtypes(include=[np.number]) \n",
    "    \n",
    "    print(f\"‚úÖ Dropped non-numeric columns. Remaining features: {len(X.columns)}\")\n",
    "\n",
    "    # 4. PREPROCESSING\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # 5. TRAIN/TEST SPLIT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 6. MODEL SELECTION\n",
    "    if args.target_type == 'affinity':\n",
    "        model = RandomForestRegressor(n_estimators=args.n_estimators, max_depth=args.max_depth)\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=args.n_estimators, max_depth=args.max_depth)\n",
    "\n",
    "    print(f\"üöÄ Fitting {args.target_type} model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 7. LOGGING FOR TUNER\n",
    "    preds = model.predict(X_test)\n",
    "    if args.target_type == 'affinity':\n",
    "        print(f\"MSE: {mean_squared_error(y_test, preds):.4f}\")\n",
    "    else:\n",
    "        print(f\"F1: {f1_score(y_test, preds):.4f}\")\n",
    "\n",
    "    # 8. SAVE OUTPUTS\n",
    "    joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    joblib.dump(imputer, os.path.join(args.model_dir, \"imputer.joblib\"))\n",
    "    print(\"‚úÖ Model and Imputer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a31f5e-fe3c-4c0c-b0cf-e5c006093113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tuner import IntegerParameter, HyperparameterTuner\n",
    "\n",
    "# 1. INITIALIZE SESSION & PERMISSIONS\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_name = 'BUCKET PLACEHOLDER'\n",
    "train_path = f\"s3://{bucket_name}/gold/train_features\"\n",
    "\n",
    "print(f\"üìç Region: {sagemaker_session.boto_region_name}\")\n",
    "print(f\"üì¶ Data Path: {train_path}\")\n",
    "\n",
    "# 2. SHARED HYPERPARAMETER RANGES\n",
    "hyperparameter_ranges = {\n",
    "    'n_estimators': IntegerParameter(50, 250),\n",
    "    'max_depth': IntegerParameter(5, 25)\n",
    "}\n",
    "\n",
    "# 3. CONFIGURE TRACK A: BINDING AFFINITY (Regression)\n",
    "estimator_affinity = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    dependencies=['requirements.txt'],  \n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge', \n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={'target_type': 'affinity'}\n",
    ")\n",
    "\n",
    "tuner_affinity = HyperparameterTuner(\n",
    "    estimator_affinity,\n",
    "    objective_metric_name='mse',\n",
    "    objective_type='Minimize',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[{'Name': 'mse', 'Regex': 'MSE: ([0-9\\\\.]+)'}],\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    base_tuning_job_name='affinity-tune' # Changed underscore to hyphen (AWS preference)\n",
    ")\n",
    "\n",
    "# 4. CONFIGURE TRACK B: BIOLOGICAL ACTIVITY (Classification)\n",
    "estimator_activity = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    dependencies=['requirements.txt'], # Added dependencies here as well!\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={'target_type': 'activity'}\n",
    ")\n",
    "\n",
    "tuner_activity = HyperparameterTuner(\n",
    "    estimator_activity,\n",
    "    objective_metric_name='f1-score',\n",
    "    objective_type='Maximize',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[{'Name': 'f1-score', 'Regex': 'F1: ([0-9\\\\.]+)'}],\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    base_tuning_job_name='activity-tune'\n",
    ")\n",
    "\n",
    "# 5. TRIGGER BOTH SEARCHES! üöÄ\n",
    "train_input = sagemaker.inputs.TrainingInput(s3_data=train_path, content_type='application/x-parquet')\n",
    "\n",
    "print(\"‚ö° Launching Affinity Tuning Job...\")\n",
    "tuner_affinity.fit({'train': train_input}, wait=False) \n",
    "\n",
    "print(\"‚ö° Launching Activity Tuning Job...\")\n",
    "tuner_activity.fit({'train': train_input}, wait=True) \n",
    "\n",
    "print(\"üéâ Both tuning jobs are in progress.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45bd1b-3280-4bfb-8168-8847b7689a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "target_bucket = 'BUCKET PLACEHOLDER'\n",
    "\n",
    "try:\n",
    "    # 1. Find the REAL region of the bucket\n",
    "    loc = s3_client.get_bucket_location(Bucket=target_bucket)['LocationConstraint']\n",
    "    actual_region = loc if loc else 'us-east-1'\n",
    "    print(f\"üåç The bucket actually lives in: {actual_region}\")\n",
    "    \n",
    "    # 2. Test if the SageMaker Role can 'List' the bucket\n",
    "    print(f\"üïµÔ∏è Testing permissions for role...\")\n",
    "    objects = s3_client.list_objects_v2(Bucket=target_bucket, Prefix='gold/train_features', MaxKeys=5)\n",
    "    \n",
    "    if 'Contents' in objects:\n",
    "        print(\"‚úÖ Success! Objects found:\")\n",
    "        for obj in objects['Contents']:\n",
    "            print(f\" - {obj['Key']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No objects found. Check the prefix/folder name.\")\n",
    "\n",
    "except ClientError as e:\n",
    "    print(f\"üö´ Permission Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8ae471-a6be-479a-b74c-5509559f2152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pyarrow\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.2.1\n",
    "joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e001aae-433f-484a-a367-2258850c8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# 1. Setup local paths to mimic SageMaker environment\n",
    "os.makedirs('model_dir', exist_ok=True)\n",
    "train_dir = 'local_data'\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "# 2. Download the data from S3 to this notebook instance\n",
    "# This proves the notebook can 'see' the data\n",
    "!aws s3 cp s3REGIONLOC/gold/train_features/data.parquet local_data/data.parquet\n",
    "\n",
    "# 3. Run the script manually to see the REAL error inside train.py\n",
    "# This is the \"Truth Test\" for your script logic\n",
    "!python train.py --n_estimators 100 --max_depth 5 --target_type affinity --train local_data/ --model_dir model_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ded55bc6-bb40-410d-ac8f-10029ff26a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model and imputer from the model_dir\"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    imputer = joblib.load(os.path.join(model_dir, \"imputer.joblib\"))\n",
    "    return {\"model\": model, \"imputer\": imputer}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse the incoming JSON data from the Agent/Lambda\"\"\"\n",
    "    if request_content_type == 'application/json':\n",
    "        data = json.loads(request_body)\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"Apply the same preprocessing and make a prediction\"\"\"\n",
    "    model = model_dict[\"model\"]\n",
    "    imputer = model_dict[\"imputer\"]\n",
    "    \n",
    "    # 1. Only keep numeric columns (matches train.py logic)\n",
    "    X = input_data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # 2. Apply the saved imputer (crucial for consistency!)\n",
    "    X_imputed = imputer.transform(X)\n",
    "    \n",
    "    # 3. Predict\n",
    "    prediction = model.predict(X_imputed)\n",
    "    return prediction.tolist()\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"Return the result as JSON\"\"\"\n",
    "    return json.dumps(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b7e98-063c-4b48-9425-a49b862dfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "from time import strftime, gmtime\n",
    "\n",
    "s3_client = boto3.client('sagemaker')\n",
    "\n",
    "def get_best_job_and_artifact(tuner_name, metric_is_minimized=True):\n",
    "    \"\"\"\n",
    "    Analyzes a tuning job and returns the Best Job Name and its S3 Model Artifact URI.\n",
    "    No more guessing S3 paths!\n",
    "    \"\"\"\n",
    "    # 1. Get the Leaderboard\n",
    "    tuner_analytics = HyperparameterTuningJobAnalytics(tuner_name)\n",
    "    df = tuner_analytics.dataframe()\n",
    "    \n",
    "    # 2. Sort to find the winner\n",
    "    # If minimizing MSE, sort Ascending (True). If maximizing F1, sort Descending (False).\n",
    "    df = df.sort_values('FinalObjectiveValue', ascending=metric_is_minimized)\n",
    "    best_job_name = df.iloc[0]['TrainingJobName']\n",
    "    best_metric_val = df.iloc[0]['FinalObjectiveValue']\n",
    "    \n",
    "    # 3. ASK AWS for the exact S3 path (The \"Pro\" Move) üõ°Ô∏è\n",
    "    describe_response = s3_client.describe_training_job(TrainingJobName=best_job_name)\n",
    "    model_artifact = describe_response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    return best_job_name, best_metric_val, model_artifact\n",
    "\n",
    "# --- 1. ANALYZE AFFINITY (Minimize MSE) ---\n",
    "print(\"üìä Analyzing Affinity Tuning Job...\")\n",
    "best_aff_name, best_aff_mse, best_aff_s3 = get_best_job_and_artifact(\n",
    "    tuner_affinity.latest_tuning_job.name, metric_is_minimized=True\n",
    ")\n",
    "print(f\"   üèÜ Best Job: {best_aff_name}\")\n",
    "print(f\"   üìâ Best MSE: {best_aff_mse:.4f}\")\n",
    "print(f\"   üì¶ Artifact: {best_aff_s3}\")\n",
    "\n",
    "# --- 2. ANALYZE ACTIVITY (Maximize F1) ---\n",
    "print(\"\\nüìä Analyzing Activity Tuning Job...\")\n",
    "best_act_name, best_act_f1, best_act_s3 = get_best_job_and_artifact(\n",
    "    tuner_activity.latest_tuning_job.name, metric_is_minimized=False\n",
    ")\n",
    "print(f\"   üèÜ Best Job: {best_act_name}\")\n",
    "print(f\"   üéØ Best F1:  {best_act_f1:.4f}\")\n",
    "print(f\"   üì¶ Artifact: {best_act_s3}\")\n",
    "\n",
    "# --- 3. DEPLOY ---\n",
    "timestamp = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "instance_type = 'ml.m5.large'\n",
    "\n",
    "print(f\"\\nüöÄ Deploying endpoints (Timestamp: {timestamp})...\")\n",
    "\n",
    "# Deploy Affinity\n",
    "affinity_model = SKLearnModel(\n",
    "    model_data=best_aff_s3, # Use the exact path we found!\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    dependencies=['requirements.txt'] \n",
    ")\n",
    "\n",
    "predictor_affinity = affinity_model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f'biotech-affinity-{timestamp}'\n",
    ")\n",
    "\n",
    "# Deploy Activity\n",
    "activity_model = SKLearnModel(\n",
    "    model_data=best_act_s3, # Use the exact path we found!\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    dependencies=['requirements.txt']\n",
    ")\n",
    "\n",
    "predictor_activity = activity_model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f'biotech-activity-{timestamp}'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Deployment Complete!\")\n",
    "print(f\"üîó Affinity Endpoint: biotech-affinity-{timestamp}\")\n",
    "print(f\"üîó Activity Endpoint: biotech-activity-{timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3e75e-9552-458a-a893-495703ded336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
